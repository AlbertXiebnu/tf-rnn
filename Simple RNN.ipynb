{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%aimport -tf\n",
    "%aimport data\n",
    "%aimport model\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data\n",
    "import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_DOCUMENT_LENGTH = 20\n",
    "VOCAB_MIN_FREQUENCY = 100\n",
    "DATA_TEST_SIZE = 5000\n",
    "\n",
    "# Model Parameters\n",
    "RNN_CELL_SIZE = 128\n",
    "EMBEDDING_DIM = 64\n",
    "\n",
    "# Training Parameters\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TRAIN_EVAL_EVERY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = data.load_reddit_data(\n",
    "    max_document_length=MAX_DOCUMENT_LENGTH,\n",
    "    min_frequency=VOCAB_MIN_FREQUENCY,\n",
    "    test_size=DATA_TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 7794\n",
      "Train Data Shape: (385367, 20)\n",
      "Dev Data Shape (5000, 20)\n"
     ]
    }
   ],
   "source": [
    "data.print_dataset_stats(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input_fn, dev_input_fn = data.create_train_dev_input_fns(ds, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_rnn(sequences, sequence_lengths):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(RNN_CELL_SIZE)\n",
    "    sequence_list = tf.unpack(sequences, axis=1)\n",
    "    return tf.nn.rnn(cell, sequence_list, dtype=tf.float32, sequence_length=sequence_lengths)\n",
    "\n",
    "model_fn = model.create_lm(\n",
    "    vocab_size=len(ds.vocab.vocabulary_),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_fn=simple_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Monitor for sampling sentences from the estimator\n",
    "sample_mon = model.SentenceSampleMonitor(\n",
    "    vocab=ds.vocab,\n",
    "    every_n_steps=TRAIN_EVAL_EVERY,\n",
    "    first_n_steps=-1)\n",
    "\n",
    "# Monitor for development set loss\n",
    "dev_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    input_fn=dev_input_fn,\n",
    "    every_n_steps=TRAIN_EVAL_EVERY)\n",
    "\n",
    "model_name = \"maxlen_{}_rnn{}_embed_{}\".format(MAX_DOCUMENT_LENGTH, RNN_CELL_SIZE, EMBEDDING_DIM)\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model_fn, model_dir=\"./checkpoints/{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Setting feature info to {'x': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(20)]), is_sparse=False), 'x_len': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16)]), is_sparse=False)}\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(19)]), is_sparse=False)\n",
      "INFO:tensorflow:Create CheckpointSaver\n",
      "INFO:tensorflow:Restored model from ./checkpoints/maxlen_20_rnn128_embed_64/model.ckpt-9302-?????-of-00001\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from model at ./checkpoints/maxlen_20_rnn128_embed_64/model.ckpt-9302-?????-of-00001\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: {'x': <tf.Tensor 'batch:0' shape=(16, 20) dtype=int64>, 'x_len': <tf.Tensor 'batch:1' shape=(16,) dtype=int64>}, required signatures: {'x': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(20)]), is_sparse=False), 'x_len': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16)]), is_sparse=False)}.\n",
      "WARNING:tensorflow:Given targets: Tensor(\"batch:2\", shape=(16, 19), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(19)]), is_sparse=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SETENCE_START\n",
      "SETENCE_START efforts\n",
      "SETENCE_START efforts Air\n",
      "SETENCE_START efforts Air wings\n",
      "SETENCE_START efforts Air wings nEvery\n",
      "SETENCE_START efforts Air wings nEvery episodes\n",
      "SETENCE_START efforts Air wings nEvery episodes Donald's\n",
      "SETENCE_START efforts Air wings nEvery episodes Donald's deal\n",
      "SETENCE_START efforts Air wings nEvery episodes Donald's deal sitting\n",
      "SETENCE_START efforts Air wings nEvery episodes Donald's deal sitting pig\n",
      "[0.00012811658, 0.00012816588, 0.00012805931, 0.0001280334, 0.00012808673, 0.00012805965, 0.00012848173, 0.00012822551, 0.00012809444, 0.00014847447]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restored model from ./checkpoints/maxlen_20_rnn128_embed_64/model.ckpt-9302-?????-of-00001\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 9302.\n",
      "INFO:tensorflow:Results after 10 steps (0.079 sec/batch): loss = 7.16262.\n",
      "INFO:tensorflow:Results after 20 steps (0.079 sec/batch): loss = 7.26977.\n",
      "INFO:tensorflow:Results after 30 steps (0.077 sec/batch): loss = 7.25516.\n",
      "INFO:tensorflow:Results after 40 steps (0.077 sec/batch): loss = 7.22125.\n",
      "INFO:tensorflow:Results after 50 steps (0.076 sec/batch): loss = 7.37126.\n",
      "INFO:tensorflow:Results after 60 steps (0.080 sec/batch): loss = 7.42404.\n",
      "INFO:tensorflow:Results after 70 steps (0.078 sec/batch): loss = 7.25589.\n",
      "INFO:tensorflow:Results after 80 steps (0.075 sec/batch): loss = 7.08676.\n",
      "INFO:tensorflow:Results after 90 steps (0.079 sec/batch): loss = 7.23791.\n",
      "INFO:tensorflow:Results after 100 steps (0.074 sec/batch): loss = 7.42615.\n",
      "INFO:tensorflow:Results after 110 steps (0.080 sec/batch): loss = 7.40851.\n",
      "INFO:tensorflow:Results after 120 steps (0.099 sec/batch): loss = 7.39033.\n",
      "INFO:tensorflow:Results after 130 steps (0.090 sec/batch): loss = 7.19903.\n",
      "INFO:tensorflow:Results after 140 steps (0.084 sec/batch): loss = 7.50422.\n",
      "INFO:tensorflow:Results after 150 steps (0.092 sec/batch): loss = 7.23506.\n",
      "INFO:tensorflow:Results after 160 steps (0.110 sec/batch): loss = 7.28951.\n",
      "INFO:tensorflow:Results after 170 steps (0.074 sec/batch): loss = 7.41865.\n",
      "INFO:tensorflow:Results after 180 steps (0.084 sec/batch): loss = 7.47367.\n",
      "INFO:tensorflow:Results after 190 steps (0.074 sec/batch): loss = 7.4167.\n",
      "INFO:tensorflow:Results after 200 steps (0.082 sec/batch): loss = 7.40176.\n",
      "INFO:tensorflow:Results after 210 steps (0.076 sec/batch): loss = 7.36153.\n",
      "INFO:tensorflow:Results after 220 steps (0.079 sec/batch): loss = 7.28189.\n",
      "INFO:tensorflow:Results after 230 steps (0.104 sec/batch): loss = 7.10578.\n",
      "INFO:tensorflow:Results after 240 steps (0.079 sec/batch): loss = 7.42997.\n",
      "INFO:tensorflow:Results after 250 steps (0.081 sec/batch): loss = 7.49783.\n",
      "INFO:tensorflow:Results after 260 steps (0.082 sec/batch): loss = 7.54173.\n",
      "INFO:tensorflow:Results after 270 steps (0.079 sec/batch): loss = 7.11934.\n",
      "INFO:tensorflow:Results after 280 steps (0.077 sec/batch): loss = 7.5622.\n",
      "INFO:tensorflow:Results after 290 steps (0.084 sec/batch): loss = 7.45491.\n",
      "INFO:tensorflow:Results after 300 steps (0.079 sec/batch): loss = 7.48169.\n",
      "INFO:tensorflow:Results after 310 steps (0.087 sec/batch): loss = 7.50599.\n",
      "INFO:tensorflow:Input queue is exhausted.\n",
      "INFO:tensorflow:Saving evaluation summary for 9302 step: loss = 7.08082\n",
      "INFO:tensorflow:Validation (step 9303): global_step = 9302, loss = 7.08082\n",
      "INFO:tensorflow:Step 9303: loss = 7.46551\n",
      "INFO:tensorflow:Saving checkpoints for 9303 into ./checkpoints/maxlen_20_rnn128_embed_64/model.ckpt.\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(input_fn=train_input_fn, steps=None, monitors=[sample_mon, dev_monitor])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
