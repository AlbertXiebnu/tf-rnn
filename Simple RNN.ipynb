{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%aimport -tf\n",
    "%aimport data\n",
    "%aimport model\n",
    "%autoreload 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import data\n",
    "import model\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.logging.set_verbosity(tf.logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "MAX_DOCUMENT_LENGTH = 20\n",
    "VOCAB_MIN_FREQUENCY = 50\n",
    "DATA_TEST_SIZE = 5000\n",
    "\n",
    "# Model Parameters\n",
    "RNN_CELL_SIZE = 256\n",
    "EMBEDDING_DIM = 128\n",
    "\n",
    "# Training Parameters\n",
    "TRAIN_BATCH_SIZE = 16\n",
    "TRAIN_EVAL_EVERY = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ds = data.load_reddit_data(\n",
    "    max_document_length=MAX_DOCUMENT_LENGTH,\n",
    "    min_frequency=VOCAB_MIN_FREQUENCY,\n",
    "    test_size=DATA_TEST_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 10803\n",
      "Train Data Shape: (385367, 20)\n",
      "Dev Data Shape (5000, 20)\n",
      "Vocabulary size 10803\n"
     ]
    }
   ],
   "source": [
    "data.print_dataset_stats(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_input_fn, dev_input_fn = data.create_train_dev_input_fns(ds, batch_size=TRAIN_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def simple_rnn(sequences, sequence_lengths):\n",
    "    cell = tf.nn.rnn_cell.GRUCell(RNN_CELL_SIZE)\n",
    "    sequence_list = tf.unpack(sequences, axis=1)\n",
    "    return tf.nn.rnn(cell, sequence_list, dtype=tf.float32, sequence_length=sequence_lengths)\n",
    "\n",
    "model_fn = model.create_language_model_rnn(\n",
    "    vocab_size=len(ds.vocab.vocabulary_),\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    rnn_fn=simple_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Monitor for sampling sentences from the estimator\n",
    "sample_mon = model.SentenceSampleMonitor(\n",
    "    vocab=ds.vocab,\n",
    "    every_n_steps=TRAIN_EVAL_EVERY,\n",
    "    first_n_steps=-1)\n",
    "\n",
    "# Monitor for development set loss\n",
    "dev_monitor = tf.contrib.learn.monitors.ValidationMonitor(\n",
    "    input_fn=dev_input_fn,\n",
    "    every_n_steps=TRAIN_EVAL_EVERY)\n",
    "\n",
    "model_name = \"maxlen_{}_rnn{}_embed_{}\".format(MAX_DOCUMENT_LENGTH, RNN_CELL_SIZE, EMBEDDING_DIM)\n",
    "estimator = tf.contrib.learn.Estimator(model_fn=model_fn, model_dir=\"./checkpoints/{}\".format(model_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Setting feature info to {'x': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(20)]), is_sparse=False), 'x_len': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16)]), is_sparse=False)}\n",
      "WARNING:tensorflow:Setting targets info to TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(19)]), is_sparse=False)\n",
      "INFO:tensorflow:Create CheckpointSaver\n",
      "INFO:tensorflow:Restored model from ./checkpoints/maxlen_20_rnn256_embed_128/model.ckpt-900-?????-of-00001\n",
      "INFO:tensorflow:Step 901: loss = 6.37077\n",
      "INFO:tensorflow:Saving checkpoints for 901 into ./checkpoints/maxlen_20_rnn256_embed_128/model.ckpt.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampling from model at ./checkpoints/maxlen_20_rnn256_embed_128/model.ckpt-901-?????-of-00001\n",
      "SENTENCE_START\n",
      "SENTENCE_START i\n",
      "SENTENCE_START i are\n",
      "SENTENCE_START i are ;\n",
      "SENTENCE_START i are ; <UNK>\n",
      "SENTENCE_START i are ; <UNK> drugs\n",
      "SENTENCE_START i are ; <UNK> drugs would\n",
      "SENTENCE_START i are ; <UNK> drugs would miles\n",
      "SENTENCE_START i are ; <UNK> drugs would miles ,\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Given features: {'x': <tf.Tensor 'batch:0' shape=(16, 20) dtype=int64>, 'x_len': <tf.Tensor 'batch:1' shape=(16,) dtype=int64>}, required signatures: {'x': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(20)]), is_sparse=False), 'x_len': TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16)]), is_sparse=False)}.\n",
      "WARNING:tensorflow:Given targets: Tensor(\"batch:2\", shape=(16, 19), dtype=int64), required signatures: TensorSignature(dtype=tf.int64, shape=TensorShape([Dimension(16), Dimension(19)]), is_sparse=False).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SENTENCE_START i are ; <UNK> drugs would miles , against\n",
      "[0.11155516, 0.0082614403, 0.012146911, 0.058830481, 9.1312548e-05, 0.0028278122, 2.9213594e-05, 0.019151369, 0.00021820559, 0.0071246373]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restored model from ./checkpoints/maxlen_20_rnn256_embed_128/model.ckpt-901-?????-of-00001\n",
      "INFO:tensorflow:Eval steps [0,inf) for training step 901.\n",
      "INFO:tensorflow:Results after 10 steps (0.192 sec/batch): loss = 5.98163.\n",
      "INFO:tensorflow:Results after 20 steps (0.184 sec/batch): loss = 6.28958.\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(input_fn=train_input_fn, steps=None, monitors=[sample_mon, dev_monitor])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
