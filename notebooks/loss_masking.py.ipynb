{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 10.8038826    6.80986166  24.33704185  21.3249836 ]\n",
      " [  0.          15.77543354  27.58551407  28.09634399]\n",
      " [  0.           0.           2.44341588  24.29099083]\n",
      " [  0.           0.           0.          20.08712196]\n",
      " [  0.           0.           0.          34.75185013]\n",
      " [  0.           0.           0.          44.74113464]\n",
      " [  0.           0.           0.          23.10951996]\n",
      " [  0.           0.           0.          20.82464027]]\n",
      "[ 10.8038826   11.29264736  18.1219902   27.15332413]\n",
      "16.843\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Number of examples\n",
    "N = 4\n",
    "# (Maximum) number of time steps in this batch\n",
    "T = 8\n",
    "RNN_DIM = 128\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The *acutal* length of the examples\n",
    "example_len = [1, 2, 3, 8]\n",
    "\n",
    "# The classes of the examples at each step (between 1 and 9, 0 means padding)\n",
    "y = np.random.randint(1, 10, [N, T])\n",
    "for i, length in enumerate(example_len):\n",
    "    y[i, length:] = 0   \n",
    "    \n",
    "# The RNN outputs\n",
    "rnn_outputs = tf.convert_to_tensor(np.random.randn(N, T, RNN_DIM), dtype=tf.float32)\n",
    "\n",
    "# Output layer weights\n",
    "W = tf.get_variable(\n",
    "    name=\"W\",\n",
    "    initializer=tf.random_normal_initializer(),\n",
    "    shape=[RNN_DIM, NUM_CLASSES])\n",
    "\n",
    "# Swap the first two dimensions (batch and time) of the outputs\n",
    "rnn_outputs_t = tf.transpose(rnn_outputs, [1, 0, 2])\n",
    "\n",
    "# For each time step, calculate the logits and probabilities per batch\n",
    "logits = tf.map_fn(lambda x: tf.batch_matmul(x, W), rnn_outputs_t, name=\"logits\")\n",
    "probs = tf.map_fn(tf.nn.softmax, logits, name=\"probs\")\n",
    "\n",
    "# Calculate the losses for each time step\n",
    "y_by_time = tf.transpose(y)\n",
    "losses = tf.map_fn(\n",
    "    lambda x: tf.nn.sparse_softmax_cross_entropy_with_logits(*x),\n",
    "    [logits, y_by_time],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# Set all losss where y=0 (padding) to 0\n",
    "mask = tf.sign(tf.to_float(y_by_time))\n",
    "masked_losses = mask * losses\n",
    "\n",
    "# Calculate the losses for the whole sequence\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, reduction_indices=0) / example_len\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\n",
    "        \"masked_losses\": masked_losses,\n",
    "        \"mean_loss_by_example\": mean_loss_by_example,\n",
    "        \"mean_loss\": mean_loss\n",
    "    },\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "\n",
    "print(result[0][\"masked_losses\"])\n",
    "print(result[0][\"mean_loss_by_example\"])\n",
    "print(result[0][\"mean_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
