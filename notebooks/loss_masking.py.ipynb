{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  2.38418551e-07   0.00000000e+00   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  6.72377443e+00   3.12269268e+01   0.00000000e+00   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  7.59313777e-02   2.27465210e+01   2.21607952e+01   0.00000000e+00\n",
      "    0.00000000e+00   0.00000000e+00   0.00000000e+00   0.00000000e+00]\n",
      " [  1.85533726e+00   1.92939949e+01   7.84722519e+00   9.50123692e+00\n",
      "    1.75973301e+01   7.04502090e-05   3.47891846e+01   2.17570434e-03]]\n",
      "[  2.38418551e-07   1.89753513e+01   1.49944153e+01   1.13608189e+01]\n",
      "11.3326\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Number of examples\n",
    "N = 4\n",
    "# (Maximum) number of time steps in this batch\n",
    "T = 8\n",
    "RNN_DIM = 128\n",
    "NUM_CLASSES = 10\n",
    "\n",
    "# The *acutal* length of the examples\n",
    "example_len = [1, 2, 3, 8]\n",
    "\n",
    "# The classes of the examples at each step (between 1 and 9, 0 means padding)\n",
    "y = np.random.randint(1, 10, [N, T])\n",
    "for i, length in enumerate(example_len):\n",
    "    y[i, length:] = 0   \n",
    "    \n",
    "# The RNN outputs\n",
    "rnn_outputs = tf.convert_to_tensor(np.random.randn(N, T, RNN_DIM), dtype=tf.float32)\n",
    "\n",
    "# Output layer weights\n",
    "W = tf.get_variable(\n",
    "    name=\"W\",\n",
    "    initializer=tf.random_normal_initializer(),\n",
    "    shape=[RNN_DIM, NUM_CLASSES])\n",
    "\n",
    "# Swap the first two dimensions (batch and time) of the outputs\n",
    "rnn_outputs_t = tf.transpose(rnn_outputs, [1, 0, 2])\n",
    "\n",
    "# For each time step, calculate the logits and probabilities per batch\n",
    "logits = tf.map_fn(lambda x: tf.batch_matmul(x, W), rnn_outputs_t, name=\"logits\")\n",
    "probs = tf.map_fn(tf.nn.softmax, logits, name=\"probs\")\n",
    "\n",
    "# Calculate the losses\n",
    "# Transpose logits back to [Batch, Time, Classes]\n",
    "logits = tf.transpose(logits, [1, 0, 2])\n",
    "losses = tf.map_fn(\n",
    "    lambda x: tf.nn.sparse_softmax_cross_entropy_with_logits(*x),\n",
    "    [logits, y],\n",
    "    dtype=tf.float32)\n",
    "\n",
    "# Set all losss where y=0 (padding) to 0\n",
    "mask = tf.sign(tf.to_float(y))\n",
    "masked_losses = mask * losses\n",
    "\n",
    "# Calculate the losses for the whole sequence\n",
    "mean_loss_by_example = tf.reduce_sum(masked_losses, reduction_indices=1) / example_len\n",
    "mean_loss = tf.reduce_mean(mean_loss_by_example)\n",
    "\n",
    "result = tf.contrib.learn.run_n(\n",
    "    {\n",
    "        \"masked_losses\": masked_losses,\n",
    "        \"mean_loss_by_example\": mean_loss_by_example,\n",
    "        \"mean_loss\": mean_loss\n",
    "    },\n",
    "    n=1,\n",
    "    feed_dict=None)\n",
    "\n",
    "print(result[0][\"masked_losses\"])\n",
    "print(result[0][\"mean_loss_by_example\"])\n",
    "print(result[0][\"mean_loss\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
